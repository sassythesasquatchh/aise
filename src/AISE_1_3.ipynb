{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import os\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from os.path import join\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH_TO_DATA = os.path.join(\"../data\", \"Task3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(os.path.join(PATH_TO_DATA, \"housing.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value', 'ocean_proximity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15480, 10)\n",
      "(5160, 10)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(full_df,shuffle = True, test_size = 0.25, random_state=17)\n",
    "train_df=train_df.copy()\n",
    "test_df=test_df.copy()\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = list(train_df.columns)\n",
    "numerical_features.remove(\"ocean_proximity\")\n",
    "numerical_features.remove(\"median_house_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_house_age = train_df[\"housing_median_age\"].max()\n",
    "train_df[\"age_clipped\"] = train_df[\"housing_median_age\"] == max_house_age\n",
    "test_df[\"age_clipped\"] = test_df[\"housing_median_age\"] == max_house_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"median_house_value_log\"] = np.log1p(train_df[\"median_house_value\"])\n",
    "test_df[\"median_house_value_log\"] = np.log1p(test_df[\"median_house_value\"])\n",
    "\n",
    "skewed_features = [\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"population\",\n",
    "    \"total_bedrooms\",\n",
    "    \"total_rooms\",\n",
    "]\n",
    "log_numerical_features = []\n",
    "for f in skewed_features:\n",
    "    train_df[f + \"_log\"] = np.log1p(train_df[f])\n",
    "    test_df[f + \"_log\"] = np.log1p(test_df[f])\n",
    "    log_numerical_features.append(f + \"_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.53336471582229"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "lin = LinearRegression()\n",
    "\n",
    "# we will train our model based on all numerical non-target features with not NaN total_bedrooms\n",
    "appropriate_columns = train_df.drop(\n",
    "    [\n",
    "        \"median_house_value\",\n",
    "        \"median_house_value_log\",\n",
    "        \"ocean_proximity\",\n",
    "        \"total_bedrooms_log\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "train_data = appropriate_columns[~pd.isnull(train_df).any(axis=1)]\n",
    "temp_train, temp_valid = train_test_split(\n",
    "    train_data, shuffle=True, test_size=0.25, random_state=17\n",
    ")\n",
    "\n",
    "lin.fit(temp_train.drop([\"total_bedrooms\"], axis=1), temp_train[\"total_bedrooms\"])\n",
    "np.sqrt(\n",
    "    mean_squared_error(\n",
    "        lin.predict(temp_valid.drop([\"total_bedrooms\"], axis=1)),\n",
    "        temp_valid[\"total_bedrooms\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_19580\\945881367.py:5: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  train_df[\"total_bedrooms\"].loc[pd.isnull(train_df).any(axis=1)] = lin.predict(\n",
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_19580\\945881367.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[\"total_bedrooms\"].loc[pd.isnull(train_df).any(axis=1)] = lin.predict(\n",
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_19580\\945881367.py:19: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  test_df[\"total_bedrooms\"].loc[pd.isnull(test_df).any(axis=1)] = lin.predict(\n",
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_19580\\945881367.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df[\"total_bedrooms\"].loc[pd.isnull(test_df).any(axis=1)] = lin.predict(\n"
     ]
    }
   ],
   "source": [
    "lin.fit(train_data.drop([\"total_bedrooms\"], axis=1), train_data[\"total_bedrooms\"])\n",
    "train_df[\"total_bedrooms_is_nan\"] = pd.isnull(train_df).any(axis=1).astype(int)\n",
    "test_df[\"total_bedrooms_is_nan\"] = pd.isnull(test_df).any(axis=1).astype(int)\n",
    "\n",
    "train_df[\"total_bedrooms\"].loc[pd.isnull(train_df).any(axis=1)] = lin.predict(\n",
    "    train_df.drop(\n",
    "        [\n",
    "            \"median_house_value\",\n",
    "            \"median_house_value_log\",\n",
    "            \"total_bedrooms\",\n",
    "            \"total_bedrooms_log\",\n",
    "            \"ocean_proximity\",\n",
    "            \"total_bedrooms_is_nan\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )[pd.isnull(train_df).any(axis=1)]\n",
    ")\n",
    "\n",
    "test_df[\"total_bedrooms\"].loc[pd.isnull(test_df).any(axis=1)] = lin.predict(\n",
    "    test_df.drop(\n",
    "        [\n",
    "            \"median_house_value\",\n",
    "            \"median_house_value_log\",\n",
    "            \"total_bedrooms\",\n",
    "            \"total_bedrooms_log\",\n",
    "            \"ocean_proximity\",\n",
    "            \"total_bedrooms_is_nan\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )[pd.isnull(test_df).any(axis=1)]\n",
    ")\n",
    "\n",
    "# linear regression can lead to negative predictions, let's change it\n",
    "test_df[\"total_bedrooms\"] = test_df[\"total_bedrooms\"].apply(lambda x: max(x, 0))\n",
    "train_df[\"total_bedrooms\"] = train_df[\"total_bedrooms\"].apply(lambda x: max(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"total_bedrooms_log\"] = np.log1p(train_df[\"total_bedrooms\"])\n",
    "test_df[\"total_bedrooms_log\"] = np.log1p(test_df[\"total_bedrooms\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_dummies = pd.get_dummies(\n",
    "    pd.concat([train_df[\"ocean_proximity\"], test_df[\"ocean_proximity\"]]),\n",
    "    drop_first=True,\n",
    ")\n",
    "dummies_names = list(ocean_proximity_dummies.columns)\n",
    "train_df = pd.concat([train_df, ocean_proximity_dummies[: train_df.shape[0]]], axis=1)\n",
    "test_df = pd.concat([test_df, ocean_proximity_dummies[train_df.shape[0] :]], axis=1)\n",
    "\n",
    "train_df = train_df.drop([\"ocean_proximity\"], axis=1)\n",
    "test_df = test_df.drop([\"ocean_proximity\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coord = [-122.4194, 37.7749]\n",
    "la_coord = [-118.2437, 34.0522]\n",
    "\n",
    "train_df[\"distance_to_SF\"] = np.sqrt(\n",
    "    (train_df[\"longitude\"] - sf_coord[0]) ** 2\n",
    "    + (train_df[\"latitude\"] - sf_coord[1]) ** 2\n",
    ")\n",
    "test_df[\"distance_to_SF\"] = np.sqrt(\n",
    "    (test_df[\"longitude\"] - sf_coord[0]) ** 2 + (test_df[\"latitude\"] - sf_coord[1]) ** 2\n",
    ")\n",
    "\n",
    "train_df[\"distance_to_LA\"] = np.sqrt(\n",
    "    (train_df[\"longitude\"] - la_coord[0]) ** 2\n",
    "    + (train_df[\"latitude\"] - la_coord[1]) ** 2\n",
    ")\n",
    "test_df[\"distance_to_LA\"] = np.sqrt(\n",
    "    (test_df[\"longitude\"] - la_coord[0]) ** 2 + (test_df[\"latitude\"] - la_coord[1]) ** 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_ml = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housing_median_age\",\n",
    "    \"total_rooms\",\n",
    "    \"total_bedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"median_house_value\",\n",
    "    \"age_clipped\",\n",
    "    \"total_bedrooms_is_nan\",\n",
    "    \"INLAND\",\n",
    "    \"ISLAND\",\n",
    "    \"NEAR BAY\",\n",
    "    \"NEAR OCEAN\",\n",
    "    \"distance_to_SF\",\n",
    "    \"distance_to_LA\",\n",
    "]\n",
    "train_df=train_df[columns_for_ml]\n",
    "test_df=test_df[columns_for_ml]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
       "       'total_bedrooms', 'population', 'households', 'median_income',\n",
       "       'median_house_value', 'age_clipped', 'total_bedrooms_is_nan', 'INLAND',\n",
       "       'ISLAND', 'NEAR BAY', 'NEAR OCEAN', 'distance_to_SF', 'distance_to_LA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummies_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['longitude',\n",
       " 'latitude',\n",
       " 'housing_median_age',\n",
       " 'total_rooms',\n",
       " 'total_bedrooms',\n",
       " 'population',\n",
       " 'households',\n",
       " 'median_income']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features_to_scale = (\n",
    "    numerical_features + [\"distance_to_SF\", \"distance_to_LA\"]\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(train_df[features_to_scale]),\n",
    "    columns=features_to_scale,\n",
    "    index=train_df.index,\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(test_df[features_to_scale]),\n",
    "    columns=features_to_scale,\n",
    "    index=test_df.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=10, random_state=17, shuffle=True)\n",
    "X = pd.concat(\n",
    "    [train_df[dummies_names+['age_clipped']], X_train_scaled],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "y = train_df[\"median_house_value\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "      <td>1.548000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-4.980225e-16</td>\n",
       "      <td>-2.698043e-15</td>\n",
       "      <td>-3.304850e-17</td>\n",
       "      <td>1.101617e-17</td>\n",
       "      <td>1.858978e-17</td>\n",
       "      <td>4.131062e-18</td>\n",
       "      <td>1.262269e-18</td>\n",
       "      <td>8.904735e-17</td>\n",
       "      <td>1.836028e-17</td>\n",
       "      <td>-9.363741e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "      <td>1.000032e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.379534e+00</td>\n",
       "      <td>-1.441623e+00</td>\n",
       "      <td>-2.191505e+00</td>\n",
       "      <td>-1.221424e+00</td>\n",
       "      <td>-1.288736e+00</td>\n",
       "      <td>-1.267593e+00</td>\n",
       "      <td>-1.308833e+00</td>\n",
       "      <td>-1.769147e+00</td>\n",
       "      <td>-1.548059e+00</td>\n",
       "      <td>-1.095508e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.114154e+00</td>\n",
       "      <td>-7.971526e-01</td>\n",
       "      <td>-8.452381e-01</td>\n",
       "      <td>-5.494133e-01</td>\n",
       "      <td>-5.774149e-01</td>\n",
       "      <td>-5.685615e-01</td>\n",
       "      <td>-5.779252e-01</td>\n",
       "      <td>-6.881189e-01</td>\n",
       "      <td>-1.067902e+00</td>\n",
       "      <td>-9.634331e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.298445e-01</td>\n",
       "      <td>-6.430401e-01</td>\n",
       "      <td>2.587558e-02</td>\n",
       "      <td>-2.333367e-01</td>\n",
       "      <td>-2.457856e-01</td>\n",
       "      <td>-2.310978e-01</td>\n",
       "      <td>-2.375387e-01</td>\n",
       "      <td>-1.778486e-01</td>\n",
       "      <td>5.525557e-01</td>\n",
       "      <td>-3.954593e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.801806e-01</td>\n",
       "      <td>9.728059e-01</td>\n",
       "      <td>6.594128e-01</td>\n",
       "      <td>2.371864e-01</td>\n",
       "      <td>2.660771e-01</td>\n",
       "      <td>2.697412e-01</td>\n",
       "      <td>2.849151e-01</td>\n",
       "      <td>4.618554e-01</td>\n",
       "      <td>7.875922e-01</td>\n",
       "      <td>1.056705e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.542497e+00</td>\n",
       "      <td>2.948248e+00</td>\n",
       "      <td>1.847295e+00</td>\n",
       "      <td>1.363325e+01</td>\n",
       "      <td>1.173372e+01</td>\n",
       "      <td>3.058523e+01</td>\n",
       "      <td>1.201110e+01</td>\n",
       "      <td>5.813177e+00</td>\n",
       "      <td>2.172428e+00</td>\n",
       "      <td>2.981499e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 5             6             7             8             9   \\\n",
       "count  1.548000e+04  1.548000e+04  1.548000e+04  1.548000e+04  1.548000e+04   \n",
       "mean  -4.980225e-16 -2.698043e-15 -3.304850e-17  1.101617e-17  1.858978e-17   \n",
       "std    1.000032e+00  1.000032e+00  1.000032e+00  1.000032e+00  1.000032e+00   \n",
       "min   -2.379534e+00 -1.441623e+00 -2.191505e+00 -1.221424e+00 -1.288736e+00   \n",
       "25%   -1.114154e+00 -7.971526e-01 -8.452381e-01 -5.494133e-01 -5.774149e-01   \n",
       "50%    5.298445e-01 -6.430401e-01  2.587558e-02 -2.333367e-01 -2.457856e-01   \n",
       "75%    7.801806e-01  9.728059e-01  6.594128e-01  2.371864e-01  2.660771e-01   \n",
       "max    2.542497e+00  2.948248e+00  1.847295e+00  1.363325e+01  1.173372e+01   \n",
       "\n",
       "                 10            11            12            13            14  \n",
       "count  1.548000e+04  1.548000e+04  1.548000e+04  1.548000e+04  1.548000e+04  \n",
       "mean   4.131062e-18  1.262269e-18  8.904735e-17  1.836028e-17 -9.363741e-17  \n",
       "std    1.000032e+00  1.000032e+00  1.000032e+00  1.000032e+00  1.000032e+00  \n",
       "min   -1.267593e+00 -1.308833e+00 -1.769147e+00 -1.548059e+00 -1.095508e+00  \n",
       "25%   -5.685615e-01 -5.779252e-01 -6.881189e-01 -1.067902e+00 -9.634331e-01  \n",
       "50%   -2.310978e-01 -2.375387e-01 -1.778486e-01  5.525557e-01 -3.954593e-01  \n",
       "75%    2.697412e-01  2.849151e-01  4.618554e-01  7.875922e-01  1.056705e+00  \n",
       "max    3.058523e+01  1.201110e+01  5.813177e+00  2.172428e+00  2.981499e+00  "
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3400</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.609553</td>\n",
       "      <td>-0.633700</td>\n",
       "      <td>0.105068</td>\n",
       "      <td>0.267195</td>\n",
       "      <td>-0.055940</td>\n",
       "      <td>0.247422</td>\n",
       "      <td>0.092293</td>\n",
       "      <td>0.424206</td>\n",
       "      <td>0.594737</td>\n",
       "      <td>-0.993357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9159</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.559735</td>\n",
       "      <td>-0.558979</td>\n",
       "      <td>-1.003622</td>\n",
       "      <td>4.990733</td>\n",
       "      <td>4.896872</td>\n",
       "      <td>5.005839</td>\n",
       "      <td>5.124210</td>\n",
       "      <td>0.419866</td>\n",
       "      <td>0.522846</td>\n",
       "      <td>-0.915681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10592</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.893517</td>\n",
       "      <td>-0.909234</td>\n",
       "      <td>-1.003622</td>\n",
       "      <td>0.353398</td>\n",
       "      <td>-0.084777</td>\n",
       "      <td>0.135827</td>\n",
       "      <td>-0.026446</td>\n",
       "      <td>1.157171</td>\n",
       "      <td>0.921515</td>\n",
       "      <td>-0.854024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.624499</td>\n",
       "      <td>-0.722431</td>\n",
       "      <td>-0.132509</td>\n",
       "      <td>-1.125025</td>\n",
       "      <td>-1.053231</td>\n",
       "      <td>-0.973875</td>\n",
       "      <td>-1.018581</td>\n",
       "      <td>-0.694198</td>\n",
       "      <td>0.653895</td>\n",
       "      <td>-1.062084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.308445</td>\n",
       "      <td>1.005496</td>\n",
       "      <td>0.896989</td>\n",
       "      <td>-0.731088</td>\n",
       "      <td>-0.673539</td>\n",
       "      <td>-0.674800</td>\n",
       "      <td>-0.694026</td>\n",
       "      <td>0.152030</td>\n",
       "      <td>-1.462103</td>\n",
       "      <td>1.152951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7132</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.724135</td>\n",
       "      <td>-0.750452</td>\n",
       "      <td>-0.686854</td>\n",
       "      <td>-0.019684</td>\n",
       "      <td>-0.260204</td>\n",
       "      <td>0.292953</td>\n",
       "      <td>-0.050194</td>\n",
       "      <td>0.903611</td>\n",
       "      <td>0.729682</td>\n",
       "      <td>-1.045329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10801</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.808826</td>\n",
       "      <td>-0.937255</td>\n",
       "      <td>0.025876</td>\n",
       "      <td>-0.529021</td>\n",
       "      <td>-0.611058</td>\n",
       "      <td>-0.856923</td>\n",
       "      <td>-0.746799</td>\n",
       "      <td>1.133901</td>\n",
       "      <td>0.887165</td>\n",
       "      <td>-0.884653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3022</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.061554</td>\n",
       "      <td>-0.222733</td>\n",
       "      <td>0.421836</td>\n",
       "      <td>0.370546</td>\n",
       "      <td>0.383829</td>\n",
       "      <td>0.321521</td>\n",
       "      <td>0.290192</td>\n",
       "      <td>-0.780164</td>\n",
       "      <td>0.031962</td>\n",
       "      <td>-0.420160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9099</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.823771</td>\n",
       "      <td>-0.470247</td>\n",
       "      <td>0.421836</td>\n",
       "      <td>-1.184811</td>\n",
       "      <td>-1.226255</td>\n",
       "      <td>-1.222955</td>\n",
       "      <td>-1.277169</td>\n",
       "      <td>-1.293716</td>\n",
       "      <td>0.644861</td>\n",
       "      <td>-0.823470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14109</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1.227298</td>\n",
       "      <td>-1.348221</td>\n",
       "      <td>0.421836</td>\n",
       "      <td>-0.234727</td>\n",
       "      <td>0.138712</td>\n",
       "      <td>-0.044511</td>\n",
       "      <td>0.168814</td>\n",
       "      <td>-0.925638</td>\n",
       "      <td>1.372831</td>\n",
       "      <td>-0.383463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0      1      2      3      4         5         6         7   \\\n",
       "3400   False  False  False  False  False  0.609553 -0.633700  0.105068   \n",
       "9159   False  False  False  False  False  0.559735 -0.558979 -1.003622   \n",
       "10592  False  False  False  False  False  0.893517 -0.909234 -1.003622   \n",
       "4281   False  False  False  False  False  0.624499 -0.722431 -0.132509   \n",
       "230    False  False   True  False  False -1.308445  1.005496  0.896989   \n",
       "7132   False  False  False  False  False  0.724135 -0.750452 -0.686854   \n",
       "10801  False  False  False  False  False  0.808826 -0.937255  0.025876   \n",
       "3022    True  False  False  False  False  0.061554 -0.222733  0.421836   \n",
       "9099    True  False  False  False  False  0.823771 -0.470247  0.421836   \n",
       "14109  False  False  False   True  False  1.227298 -1.348221  0.421836   \n",
       "\n",
       "             8         9         10        11        12        13        14  \n",
       "3400   0.267195 -0.055940  0.247422  0.092293  0.424206  0.594737 -0.993357  \n",
       "9159   4.990733  4.896872  5.005839  5.124210  0.419866  0.522846 -0.915681  \n",
       "10592  0.353398 -0.084777  0.135827 -0.026446  1.157171  0.921515 -0.854024  \n",
       "4281  -1.125025 -1.053231 -0.973875 -1.018581 -0.694198  0.653895 -1.062084  \n",
       "230   -0.731088 -0.673539 -0.674800 -0.694026  0.152030 -1.462103  1.152951  \n",
       "7132  -0.019684 -0.260204  0.292953 -0.050194  0.903611  0.729682 -1.045329  \n",
       "10801 -0.529021 -0.611058 -0.856923 -0.746799  1.133901  0.887165 -0.884653  \n",
       "3022   0.370546  0.383829  0.321521  0.290192 -0.780164  0.031962 -0.420160  \n",
       "9099  -1.184811 -1.226255 -1.222955 -1.277169 -1.293716  0.644861 -0.823470  \n",
       "14109 -0.234727  0.138712 -0.044511  0.168814 -0.925638  1.372831 -0.383463  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=15, step=1)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        bool\n",
       "1        bool\n",
       "2        bool\n",
       "3        bool\n",
       "4        bool\n",
       "5     float64\n",
       "6     float64\n",
       "7     float64\n",
       "8     float64\n",
       "9     float64\n",
       "10    float64\n",
       "11    float64\n",
       "12    float64\n",
       "13    float64\n",
       "14    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.4242,  0.5947, -0.9934],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.4199,  0.5228, -0.9157],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  1.1572,  0.9215, -0.8540],\n",
       "        ...,\n",
       "        [ 1.0000,  0.0000,  0.0000,  ..., -0.6775,  0.9901, -0.6775],\n",
       "        [ 1.0000,  0.0000,  0.0000,  ..., -0.4163, -0.5236,  0.2462],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.1134,  0.8863, -0.8908]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(X.values.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dimension, output_dimension, n_hidden_layers, neurons, retrain_seed, output_activation=None\n",
    "    ):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        # self.activation = nn.Tanh()\n",
    "        # self.activation = nn.LeakyReLU()\n",
    "        self.activation=nn.ReLU()\n",
    "        # self.output_activation=nn.ReLU()\n",
    "        if output_activation is None:\n",
    "            self.output_activation = nn.Identity()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_activation(self.output_layer(x))\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain(\"tanh\")\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalHousing:\n",
    "    def __init__(self, n_hidden_layers, n_neurons, train_df, target_df,X_valid,y_valid, seed):\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.seed=seed\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_tensor = torch.tensor(train_df.values.astype(np.float32),dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(self.scale_targets(target_df), dtype=torch.float32)\n",
    "        self.data = DataLoader(\n",
    "            torch.utils.data.TensorDataset(train_tensor, target_tensor),\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "        self.model=NeuralNet(\n",
    "            input_dimension=train_df.shape[1],\n",
    "            output_dimension=1,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            neurons=n_neurons,\n",
    "            retrain_seed=self.seed\n",
    "\n",
    "        ).to(self.device)\n",
    "        self.X=train_df\n",
    "        self.y=target_df\n",
    "\n",
    "        self.X_valid=X_valid\n",
    "        self.y_valid=y_valid\n",
    "\n",
    "        self.metadata_file=join(\"../logs\",\"task_3\" ,\"metadata.json\")\n",
    "\n",
    "    def compute_loss(self, inputs, targets, verbose=True):\n",
    "        preds=self.model(inputs)\n",
    "        # targets=targets/10000\n",
    "        res=targets-preds\n",
    "        loss=torch.mean(res**2)\n",
    "        # if verbose: print(\"Total loss: \", round(loss.item(), 4))\n",
    "        return loss\n",
    "\n",
    "    def save(self, loss_history):\n",
    "        filename=join(\"..\",\"models\", \"task_3\", datetime.datetime.now().strftime(\"%m-%d %H:%M:%S\")+\".pt\")\n",
    "        salient_info={}\n",
    "        salient_info[\"n_hidden_layers\"]=self.n_hidden_layers\n",
    "        salient_info[\"n_neurons\"]=self.n_neurons\n",
    "        salient_info[\"final_loss\"]=loss_history[-1]\n",
    "        salient_info[\"min_loss\"]=min(loss_history)\n",
    "        salient_info[\"model_path\"]=filename\n",
    "        salient_info[\"seed\"]=self.seed\n",
    "\n",
    "        torch.save(self.model.state_dict(),filename )\n",
    "        with open(self.metadata_file, \"a\") as f:\n",
    "            json.dump(salient_info, f)\n",
    "\n",
    "    def scale_targets(self, target_df):\n",
    "        y=target_df.values.astype(np.float32)\n",
    "        self.scaler=StandardScaler()\n",
    "        return(self.scaler.fit_transform(y.reshape(-1, 1)))\n",
    "    \n",
    "    # def validate(self, X,y):\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "    #         targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "    #         loss=self.compute_loss(inputs, targets)\n",
    "    #         return loss\n",
    "    \n",
    "    def RMSE(self, validation=True):\n",
    "        if validation:\n",
    "            X=self.X_valid\n",
    "            y=self.y_valid\n",
    "        else:\n",
    "            X=self.X\n",
    "            y=self.y\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "            # targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "            preds=self.model(inputs)\n",
    "            preds=self.scaler.inverse_transform(preds.cpu().numpy())\n",
    "            loss=np.sqrt(np.mean((y.values-preds)**2))\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model: CalHousing, num_epochs, optimizer, verbose=True):\n",
    "    history=list()\n",
    "    device=model.device\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if verbose: print(\"################################ \",epoch,\" ##################################\")\n",
    "\n",
    "        for inputs, targets in model.data:\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                loss=model.compute_loss(inputs.to(device), targets.to(device), verbose=True)\n",
    "                loss.backward()\n",
    "                history.append(loss.item())\n",
    "                return loss\n",
    "            optimizer.step(closure)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            # inputs = inputs.to(device)\n",
    "            # targets = targets.to(device)\n",
    "            # optimizer.zero_grad()\n",
    "            # if epoch==378:\n",
    "            #     pass\n",
    "            # loss = model.compute_loss(inputs, targets, verbose=False)\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            \n",
    "        print(\"Epoch: \", epoch, \" Loss: \", history[-1])\n",
    "        print(\"Validation RMSE: \", model.RMSE(validation=True))\n",
    "        print(\"Training RMSE: \", model.RMSE(validation=False))\n",
    "        # if history[-1]<0.09:\n",
    "        #     break\n",
    "    print('Final Loss ', history[-1])\n",
    "    model.save(history)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = pd.concat(\n",
    "    [test_df[dummies_names+['age_clipped']], X_test_scaled],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "y_valid = test_df[\"median_house_value\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################  0  ##################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0  Loss:  0.1963653266429901\n",
      "Validation RMSE:  150131.431750867\n",
      "Training RMSE:  151673.9960430337\n",
      "################################  1  ##################################\n",
      "Epoch:  1  Loss:  0.17288903892040253\n",
      "Validation RMSE:  151520.91623238302\n",
      "Training RMSE:  152863.1366218944\n",
      "################################  2  ##################################\n",
      "Epoch:  2  Loss:  0.15024423599243164\n",
      "Validation RMSE:  153745.83275058033\n",
      "Training RMSE:  155093.56597727272\n",
      "################################  3  ##################################\n",
      "Epoch:  3  Loss:  0.13761070370674133\n",
      "Validation RMSE:  152620.78957258537\n",
      "Training RMSE:  154147.3376537473\n",
      "################################  4  ##################################\n",
      "Epoch:  4  Loss:  0.14419853687286377\n",
      "Validation RMSE:  152355.4460613585\n",
      "Training RMSE:  153980.503952152\n",
      "################################  5  ##################################\n",
      "Epoch:  5  Loss:  0.142704576253891\n",
      "Validation RMSE:  152119.96622653076\n",
      "Training RMSE:  153711.7971765022\n",
      "################################  6  ##################################\n",
      "Epoch:  6  Loss:  0.13268610835075378\n",
      "Validation RMSE:  151913.94652374723\n",
      "Training RMSE:  153745.17676024247\n",
      "################################  7  ##################################\n",
      "Epoch:  7  Loss:  0.12861208617687225\n",
      "Validation RMSE:  152901.7661747005\n",
      "Training RMSE:  154702.65776147955\n",
      "################################  8  ##################################\n",
      "Epoch:  8  Loss:  0.12777718901634216\n",
      "Validation RMSE:  153698.36912941822\n",
      "Training RMSE:  155492.68102549855\n",
      "################################  9  ##################################\n",
      "Epoch:  9  Loss:  0.11880342662334442\n",
      "Validation RMSE:  153399.52228729174\n",
      "Training RMSE:  155213.12630094355\n",
      "################################  10  ##################################\n",
      "Epoch:  10  Loss:  0.12118866294622421\n",
      "Validation RMSE:  154718.2253619035\n",
      "Training RMSE:  156254.8600645316\n",
      "################################  11  ##################################\n",
      "Epoch:  11  Loss:  0.10954274982213974\n",
      "Validation RMSE:  154237.01469290085\n",
      "Training RMSE:  155847.2371009427\n",
      "################################  12  ##################################\n",
      "Epoch:  12  Loss:  0.11253143101930618\n",
      "Validation RMSE:  155908.33516861877\n",
      "Training RMSE:  157242.6428066028\n",
      "################################  13  ##################################\n",
      "Epoch:  13  Loss:  0.11054916679859161\n",
      "Validation RMSE:  154175.4660306386\n",
      "Training RMSE:  155916.27627077265\n",
      "################################  14  ##################################\n",
      "Epoch:  14  Loss:  0.10247690975666046\n",
      "Validation RMSE:  154987.95533903278\n",
      "Training RMSE:  156615.08366235567\n",
      "################################  15  ##################################\n",
      "Epoch:  15  Loss:  0.10404005646705627\n",
      "Validation RMSE:  154421.9047373901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[234], line 17\u001b[0m\n\u001b[0;32m      6\u001b[0m optimizer_ADAM \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m      7\u001b[0m                             lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(lr))\n\u001b[0;32m      9\u001b[0m optimizer_LBFGS \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mLBFGS(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m     10\u001b[0m                               lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     11\u001b[0m                               max_iter\u001b[38;5;241m=\u001b[39mmax_iter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m                               line_search_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrong_wolfe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m                               tolerance_change\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mfinfo(\u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39meps)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_ADAM\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[232], line 33\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, num_epochs, optimizer, verbose)\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation RMSE: \u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m.\u001b[39mRMSE(validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining RMSE: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRMSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# if history[-1]<0.09:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinal Loss \u001b[39m\u001b[38;5;124m'\u001b[39m, history[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[231], line 78\u001b[0m, in \u001b[0;36mCalHousing.RMSE\u001b[1;34m(self, validation)\u001b[0m\n\u001b[0;32m     76\u001b[0m preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(inputs)\n\u001b[0;32m     77\u001b[0m preds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39minverse_transform(preds\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 78\u001b[0m loss\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\patri\\OneDrive\\Patricks OneDrive Share\\Documents\\ETHZ\\Fruhling_23\\AISE\\venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\patri\\OneDrive\\Patricks OneDrive Share\\Documents\\ETHZ\\Fruhling_23\\AISE\\venv\\Lib\\site-packages\\numpy\\core\\_methods.py:118\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    115\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m mu\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    116\u001b[0m         is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m ret \u001b[38;5;241m=\u001b[39m umr_sum(arr, axis, dtype, out, keepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _no_nep50_warning():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# data=CalHousingDataset(X_train_scaled, X_test_scaled)\n",
    "model=CalHousing(6,50,X,y, X_valid,y_valid,13)\n",
    "max_iter = 50000\n",
    "num_epochs=500\n",
    "lr=0.001\n",
    "optimizer_ADAM = optim.Adam(model.model.parameters(),\n",
    "                            lr=float(lr))\n",
    "\n",
    "optimizer_LBFGS = optim.LBFGS(model.model.parameters(),\n",
    "                              lr=float(0.5),\n",
    "                              max_iter=max_iter,\n",
    "                              max_eval=50000,\n",
    "                              history_size=150,\n",
    "                              line_search_fn=\"strong_wolfe\",\n",
    "                              tolerance_change=1.0 * np.finfo(float).eps)\n",
    "\n",
    "fit(model, num_epochs, optimizer_ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  184692.19701387145\n",
      "Validation loss:  185961.63912275404\n"
     ]
    }
   ],
   "source": [
    "X_valid = pd.concat(\n",
    "    [test_df[dummies_names+['age_clipped']], X_test_scaled],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "y_valid = test_df[\"median_house_value\"].reset_index(drop=True)\n",
    "print(\"Training loss: \", model.validate_RMSE(X,y))\n",
    "print(\"Validation loss: \",model.validate_RMSE(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155256.7839733259\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs=torch.tensor(X_valid.values.astype(np.float32),dtype=torch.float32).to(model.device)\n",
    "    targets=torch.tensor(model.scale_targets(y_valid),dtype=torch.float32).to(model.device)\n",
    "    preds=model.model(inputs)\n",
    "    preds=model.scaler.inverse_transform(preds.cpu().numpy())\n",
    "    loss=np.sqrt(np.mean((y_valid.values-preds)**2))\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156918.128669661\n"
     ]
    }
   ],
   "source": [
    "model.model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(model.device)\n",
    "    targets=torch.tensor(model.scale_targets(y),dtype=torch.float32).to(model.device)\n",
    "    preds=model.model(inputs)\n",
    "    preds=model.scaler.inverse_transform(preds.cpu().numpy())\n",
    "    loss=np.sqrt(np.mean((y.values-preds)**2))\n",
    "    print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
