{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import os\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from os.path import join\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH_TO_DATA = os.path.join(\"../data\", \"Task3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(os.path.join(PATH_TO_DATA, \"housing.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['longitude', 'latitude', 'housing_median_age', 'total_rooms', 'total_bedrooms', 'population', 'households', 'median_income']\n"
     ]
    }
   ],
   "source": [
    "numerical_features=list(full_df.columns)\n",
    "numerical_features.remove(\"ocean_proximity\")\n",
    "numerical_features.remove(\"median_house_value\")\n",
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_house_age = full_df[\"housing_median_age\"].max()\n",
    "full_df[\"age_clipped\"]=full_df[\"housing_median_age\"] == max_house_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df[\"median_house_value_log\"] = np.log1p(full_df[\"median_house_value\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_features = [\n",
    "    \"households\",\n",
    "    \"median_income\",\n",
    "    \"population\",\n",
    "    \"total_bedrooms\",\n",
    "    \"total_rooms\",\n",
    "]\n",
    "log_numerical_features = []\n",
    "for f in skewed_features:\n",
    "    full_df[f + \"_log\"] = np.log1p(full_df[f])\n",
    "    log_numerical_features.append(f + \"_log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_30208\\3807254.py:16: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  full_df[\"total_bedrooms\"].loc[pd.isnull(full_df).any(axis=1)] = lin.predict(\n",
      "C:\\Users\\patri\\AppData\\Local\\Temp\\ipykernel_30208\\3807254.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  full_df[\"total_bedrooms\"].loc[pd.isnull(full_df).any(axis=1)] = lin.predict(\n",
      "c:\\Users\\patri\\OneDrive\\Patricks OneDrive Share\\Documents\\ETHZ\\Fruhling_23\\AISE\\venv\\Lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "lin = LinearRegression()\n",
    "\n",
    "# we will train our model based on all numerical non-target features with not NaN total_bedrooms\n",
    "appropriate_columns = full_df.drop(\n",
    "    [\n",
    "        \"median_house_value\",\n",
    "        \"median_house_value_log\",\n",
    "        \"ocean_proximity\",\n",
    "        \"total_bedrooms_log\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "train_data = appropriate_columns[~pd.isnull(full_df).any(axis=1)]\n",
    "lin.fit(train_data.drop([\"total_bedrooms\"], axis=1), train_data[\"total_bedrooms\"])\n",
    "full_df[\"total_bedrooms_is_nan\"] = pd.isnull(full_df).any(axis=1).astype(int)\n",
    "full_df[\"total_bedrooms\"].loc[pd.isnull(full_df).any(axis=1)] = lin.predict(\n",
    "    full_df.drop(\n",
    "        [\n",
    "            \"median_house_value\",\n",
    "            \"median_house_value_log\",\n",
    "            \"total_bedrooms\",\n",
    "            \"total_bedrooms_log\",\n",
    "            \"ocean_proximity\",\n",
    "            \"total_bedrooms_is_nan\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )[pd.isnull(full_df).any(axis=1)]\n",
    ")\n",
    "full_df['total_bedrooms_log']=np.log1p(full_df['total_bedrooms'])\n",
    "full_df=full_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20639 entries, 0 to 20639\n",
      "Data columns (total 18 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   longitude               20639 non-null  float64\n",
      " 1   latitude                20639 non-null  float64\n",
      " 2   housing_median_age      20639 non-null  float64\n",
      " 3   total_rooms             20639 non-null  float64\n",
      " 4   total_bedrooms          20639 non-null  float64\n",
      " 5   population              20639 non-null  float64\n",
      " 6   households              20639 non-null  float64\n",
      " 7   median_income           20639 non-null  float64\n",
      " 8   median_house_value      20639 non-null  float64\n",
      " 9   ocean_proximity         20639 non-null  object \n",
      " 10  age_clipped             20639 non-null  bool   \n",
      " 11  median_house_value_log  20639 non-null  float64\n",
      " 12  households_log          20639 non-null  float64\n",
      " 13  median_income_log       20639 non-null  float64\n",
      " 14  population_log          20639 non-null  float64\n",
      " 15  total_bedrooms_log      20639 non-null  float64\n",
      " 16  total_rooms_log         20639 non-null  float64\n",
      " 17  total_bedrooms_is_nan   20639 non-null  int32  \n",
      "dtypes: bool(1), float64(15), int32(1), object(1)\n",
      "memory usage: 2.8+ MB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_proximity_dummies = pd.get_dummies(\n",
    "    full_df[\"ocean_proximity\"],\n",
    "    drop_first=True,\n",
    ")\n",
    "dummies_names = list(ocean_proximity_dummies.columns)\n",
    "full_df = pd.concat([full_df, ocean_proximity_dummies[: full_df.shape[0]]], axis=1)\n",
    "\n",
    "full_df = full_df.drop([\"ocean_proximity\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_coord = [-122.4194, 37.7749]\n",
    "la_coord = [-118.2437, 34.0522]\n",
    "\n",
    "full_df[\"distance_to_SF\"] = np.sqrt(\n",
    "    (full_df[\"longitude\"] - sf_coord[0]) ** 2\n",
    "    + (full_df[\"latitude\"] - sf_coord[1]) ** 2\n",
    ")\n",
    "\n",
    "full_df[\"distance_to_LA\"] = np.sqrt(\n",
    "    (full_df[\"longitude\"] - la_coord[0]) ** 2\n",
    "    + (full_df[\"latitude\"] - la_coord[1]) ** 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_to_scale = (\n",
    "#     numerical_features + log_numerical_features+[\"distance_to_SF\", \"distance_to_LA\"]\n",
    "# )\n",
    "\n",
    "features_to_scale = (\n",
    "   log_numerical_features+[\"distance_to_SF\", \"distance_to_LA\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_features = pd.DataFrame(\n",
    "    scaler.fit_transform(full_df[features_to_scale]),\n",
    "    columns=features_to_scale,\n",
    "    index= full_df.index,\n",
    ")\n",
    "\n",
    "X=pd.concat([full_df[dummies_names+['age_clipped']], scaled_features], axis=1, ignore_index = True)\n",
    "y=full_df[\"median_house_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.to_numpy(dtype=np.float32)\n",
    "y=y.to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=full_df.drop(columns=['median_house_value', 'ocean_proximity']).to_numpy(dtype=np.float32)\n",
    "# y=full_df['median_house_value'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).view(-1, 1).float()\n",
    "\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).view(-1, 1).float()\n",
    "\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dimension, output_dimension, n_hidden_layers, neurons, retrain_seed, output_activation=None\n",
    "    ):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        # self.activation = nn.Tanh()\n",
    "        # self.activation = nn.LeakyReLU()\n",
    "        self.activation=nn.ReLU()\n",
    "        # self.output_activation=nn.ReLU()\n",
    "        if output_activation is None:\n",
    "            self.output_activation = nn.Identity()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_activation(self.output_layer(x))\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain(\"tanh\")\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalHousing:\n",
    "    def __init__(self, n_hidden_layers, n_neurons, train_df, target_df,X_valid,y_valid, seed):\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.seed=seed\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_tensor = torch.tensor(train_df.values.astype(np.float32),dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(self.scale_targets(target_df), dtype=torch.float32)\n",
    "        self.data = DataLoader(\n",
    "            torch.utils.data.TensorDataset(train_tensor, target_tensor),\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "        self.model=NeuralNet(\n",
    "            input_dimension=train_df.shape[1],\n",
    "            output_dimension=1,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            neurons=n_neurons,\n",
    "            retrain_seed=self.seed\n",
    "\n",
    "        ).to(self.device)\n",
    "        self.X=train_df\n",
    "        self.y=target_df\n",
    "\n",
    "        self.X_valid=X_valid\n",
    "        self.y_valid=y_valid\n",
    "\n",
    "        self.metadata_file=join(\"../logs\",\"task_3\" ,\"metadata.json\")\n",
    "\n",
    "    def compute_loss(self, inputs, targets, verbose=True):\n",
    "        preds=self.model(inputs)\n",
    "        # targets=targets/10000\n",
    "        res=targets-preds\n",
    "        loss=torch.mean(res**2)\n",
    "        # if verbose: print(\"Total loss: \", round(loss.item(), 4))\n",
    "        return loss\n",
    "\n",
    "    def save(self, loss_history):\n",
    "        filename=join(\"..\",\"models\", \"task_3\", datetime.datetime.now().strftime(\"%m-%d %H:%M:%S\")+\".pt\")\n",
    "        salient_info={}\n",
    "        salient_info[\"n_hidden_layers\"]=self.n_hidden_layers\n",
    "        salient_info[\"n_neurons\"]=self.n_neurons\n",
    "        salient_info[\"final_loss\"]=loss_history[-1]\n",
    "        salient_info[\"min_loss\"]=min(loss_history)\n",
    "        salient_info[\"model_path\"]=filename\n",
    "        salient_info[\"seed\"]=self.seed\n",
    "\n",
    "        torch.save(self.model.state_dict(),filename )\n",
    "        with open(self.metadata_file, \"a\") as f:\n",
    "            json.dump(salient_info, f)\n",
    "\n",
    "    def scale_targets(self, target_df):\n",
    "        y=target_df.values.astype(np.float32)\n",
    "        self.scaler=StandardScaler()\n",
    "        return(self.scaler.fit_transform(y.reshape(-1, 1)))\n",
    "    \n",
    "    # def validate(self, X,y):\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "    #         targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "    #         loss=self.compute_loss(inputs, targets)\n",
    "    #         return loss\n",
    "    \n",
    "    def RMSE(self, validation=True):\n",
    "        if validation:\n",
    "            X=self.X_valid\n",
    "            y=self.y_valid\n",
    "        else:\n",
    "            X=self.X\n",
    "            y=self.y\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "            # targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "            preds=self.model(inputs)\n",
    "            preds=self.scaler.inverse_transform(preds.cpu().numpy())\n",
    "            loss=np.sqrt(np.mean((y.values-preds)**2))\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=NeuralNet(12,1,4,40,13)\n",
    "model.train()\n",
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 1500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "def train(model_inp, num_epochs = num_epochs):\n",
    "    optimizer = torch.optim.RMSprop(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            # forward pass\n",
    "            outputs = model_inp(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[1500] running accumulative loss across all batches: 810669977632768.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21]/[1500] running accumulative loss across all batches: 279391773261824.000\n",
      "Epoch [41]/[1500] running accumulative loss across all batches: 89194355277824.000\n",
      "Epoch [61]/[1500] running accumulative loss across all batches: 68248307572736.000\n",
      "Epoch [81]/[1500] running accumulative loss across all batches: 60780762267648.000\n",
      "Epoch [101]/[1500] running accumulative loss across all batches: 57238434209792.000\n",
      "Epoch [121]/[1500] running accumulative loss across all batches: 55629554581504.000\n",
      "Epoch [141]/[1500] running accumulative loss across all batches: 54811090812928.000\n",
      "Epoch [161]/[1500] running accumulative loss across all batches: 54294554697728.000\n",
      "Epoch [181]/[1500] running accumulative loss across all batches: 53910180880384.000\n",
      "Epoch [201]/[1500] running accumulative loss across all batches: 53595560861696.000\n",
      "Epoch [221]/[1500] running accumulative loss across all batches: 53317472026624.000\n",
      "Epoch [241]/[1500] running accumulative loss across all batches: 53069982007296.000\n",
      "Epoch [261]/[1500] running accumulative loss across all batches: 52850340225024.000\n",
      "Epoch [281]/[1500] running accumulative loss across all batches: 52644402569216.000\n",
      "Epoch [301]/[1500] running accumulative loss across all batches: 52463841288192.000\n",
      "Epoch [321]/[1500] running accumulative loss across all batches: 52287242084352.000\n",
      "Epoch [341]/[1500] running accumulative loss across all batches: 52123163295744.000\n",
      "Epoch [361]/[1500] running accumulative loss across all batches: 51958023266304.000\n",
      "Epoch [381]/[1500] running accumulative loss across all batches: 51807203762176.000\n",
      "Epoch [401]/[1500] running accumulative loss across all batches: 51645676642304.000\n",
      "Epoch [421]/[1500] running accumulative loss across all batches: 51483024744448.000\n",
      "Epoch [441]/[1500] running accumulative loss across all batches: 51329722515456.000\n",
      "Epoch [461]/[1500] running accumulative loss across all batches: 51185721032704.000\n",
      "Epoch [481]/[1500] running accumulative loss across all batches: 51046688079872.000\n",
      "Epoch [501]/[1500] running accumulative loss across all batches: 50905243787264.000\n",
      "Epoch [521]/[1500] running accumulative loss across all batches: 50773349916672.000\n",
      "Epoch [541]/[1500] running accumulative loss across all batches: 50646922772480.000\n",
      "Epoch [561]/[1500] running accumulative loss across all batches: 50510255169536.000\n",
      "Epoch [581]/[1500] running accumulative loss across all batches: 50376350449664.000\n",
      "Epoch [601]/[1500] running accumulative loss across all batches: 50228119445504.000\n",
      "Epoch [621]/[1500] running accumulative loss across all batches: 50094460203008.000\n",
      "Epoch [641]/[1500] running accumulative loss across all batches: 49959322861568.000\n",
      "Epoch [661]/[1500] running accumulative loss across all batches: 49823605006336.000\n",
      "Epoch [681]/[1500] running accumulative loss across all batches: 49686112960512.000\n",
      "Epoch [701]/[1500] running accumulative loss across all batches: 49553511120896.000\n",
      "Epoch [721]/[1500] running accumulative loss across all batches: 49435055333376.000\n",
      "Epoch [741]/[1500] running accumulative loss across all batches: 49296955506688.000\n",
      "Epoch [761]/[1500] running accumulative loss across all batches: 49179853627392.000\n",
      "Epoch [781]/[1500] running accumulative loss across all batches: 49062278602752.000\n",
      "Epoch [801]/[1500] running accumulative loss across all batches: 48941877141504.000\n",
      "Epoch [821]/[1500] running accumulative loss across all batches: 48819359899648.000\n",
      "Epoch [841]/[1500] running accumulative loss across all batches: 48684398977024.000\n",
      "Epoch [861]/[1500] running accumulative loss across all batches: 48573099909120.000\n",
      "Epoch [881]/[1500] running accumulative loss across all batches: 48443781062656.000\n",
      "Epoch [901]/[1500] running accumulative loss across all batches: 48355844800512.000\n",
      "Epoch [921]/[1500] running accumulative loss across all batches: 48244611350528.000\n",
      "Epoch [941]/[1500] running accumulative loss across all batches: 48152537751552.000\n",
      "Epoch [961]/[1500] running accumulative loss across all batches: 48043277598720.000\n",
      "Epoch [981]/[1500] running accumulative loss across all batches: 47952501153792.000\n",
      "Epoch [1001]/[1500] running accumulative loss across all batches: 47856707260416.000\n",
      "Epoch [1021]/[1500] running accumulative loss across all batches: 47759852486656.000\n",
      "Epoch [1041]/[1500] running accumulative loss across all batches: 47676348784640.000\n",
      "Epoch [1061]/[1500] running accumulative loss across all batches: 47591065321472.000\n",
      "Epoch [1081]/[1500] running accumulative loss across all batches: 47518422892544.000\n",
      "Epoch [1101]/[1500] running accumulative loss across all batches: 47440981819392.000\n",
      "Epoch [1121]/[1500] running accumulative loss across all batches: 47376551927808.000\n",
      "Epoch [1141]/[1500] running accumulative loss across all batches: 47309109731328.000\n",
      "Epoch [1161]/[1500] running accumulative loss across all batches: 47244104769536.000\n",
      "Epoch [1181]/[1500] running accumulative loss across all batches: 47175969435648.000\n",
      "Epoch [1201]/[1500] running accumulative loss across all batches: 47117280124928.000\n",
      "Epoch [1221]/[1500] running accumulative loss across all batches: 47058428919808.000\n",
      "Epoch [1241]/[1500] running accumulative loss across all batches: 46994459369472.000\n",
      "Epoch [1261]/[1500] running accumulative loss across all batches: 46931640860672.000\n",
      "Epoch [1281]/[1500] running accumulative loss across all batches: 46882331254784.000\n",
      "Epoch [1301]/[1500] running accumulative loss across all batches: 46822758473728.000\n",
      "Epoch [1321]/[1500] running accumulative loss across all batches: 46772592689152.000\n",
      "Epoch [1341]/[1500] running accumulative loss across all batches: 46717794951168.000\n",
      "Epoch [1361]/[1500] running accumulative loss across all batches: 46666858815488.000\n",
      "Epoch [1381]/[1500] running accumulative loss across all batches: 46617492180992.000\n",
      "Epoch [1401]/[1500] running accumulative loss across all batches: 46559981076480.000\n",
      "Epoch [1421]/[1500] running accumulative loss across all batches: 46508843679744.000\n",
      "Epoch [1441]/[1500] running accumulative loss across all batches: 46460017410048.000\n",
      "Epoch [1461]/[1500] running accumulative loss across all batches: 46415993946112.000\n",
      "Epoch [1481]/[1500] running accumulative loss across all batches: 46359450386432.000\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  57505.145\n",
      "Training:  56603.266\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "print(\"Validation: \", err)\n",
    "print(\"Training: \", np.sqrt(mean_squared_error(model(X_train).detach().numpy(), y_train.detach().numpy())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
