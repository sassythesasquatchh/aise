{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import os\n",
    "import torch # type: ignore\n",
    "import torch.nn as nn # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "from os.path import join\n",
    "import datetime\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "PATH_TO_DATA = os.path.join(\"../data\", \"Task3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 500\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(os.path.join(PATH_TO_DATA, \"housing.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df=full_df.dropna()\n",
    "X=full_df.drop(columns=['median_house_value', 'ocean_proximity']).to_numpy(dtype=np.float32)\n",
    "y=full_df['median_house_value'].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train).float()\n",
    "y_train = torch.tensor(y_train).view(-1, 1).float()\n",
    "\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_test = torch.tensor(y_test).view(-1, 1).float()\n",
    "\n",
    "datasets = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, input_dimension, output_dimension, n_hidden_layers, neurons, retrain_seed, output_activation=None\n",
    "    ):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # Number of input dimensions n\n",
    "        self.input_dimension = input_dimension\n",
    "        # Number of output dimensions m\n",
    "        self.output_dimension = output_dimension\n",
    "        # Number of neurons per layer\n",
    "        self.neurons = neurons\n",
    "        # Number of hidden layers\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        # Activation function\n",
    "        # self.activation = nn.Tanh()\n",
    "        # self.activation = nn.LeakyReLU()\n",
    "        self.activation=nn.ReLU()\n",
    "        # self.output_activation=nn.ReLU()\n",
    "        if output_activation is None:\n",
    "            self.output_activation = nn.Identity()\n",
    "\n",
    "        self.input_layer = nn.Linear(self.input_dimension, self.neurons)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(self.neurons, self.neurons) for _ in range(n_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(self.neurons, self.output_dimension)\n",
    "        self.retrain_seed = retrain_seed\n",
    "        # Random Seed for weight initialization\n",
    "        self.init_xavier()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function performs the set of affine and non-linear transformations defining the network\n",
    "        # (see equation above)\n",
    "        x = self.activation(self.input_layer(x))\n",
    "        for k, l in enumerate(self.hidden_layers):\n",
    "            x = self.activation(l(x))\n",
    "        return self.output_activation(self.output_layer(x))\n",
    "\n",
    "    def init_xavier(self):\n",
    "        torch.manual_seed(self.retrain_seed)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear and m.weight.requires_grad and m.bias.requires_grad:\n",
    "                g = nn.init.calculate_gain(\"tanh\")\n",
    "                torch.nn.init.xavier_uniform_(m.weight, gain=g)\n",
    "                # torch.nn.init.xavier_normal_(m.weight, gain=g)\n",
    "                m.bias.data.fill_(0)\n",
    "\n",
    "        self.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalHousing:\n",
    "    def __init__(self, n_hidden_layers, n_neurons, train_df, target_df,X_valid,y_valid, seed):\n",
    "        self.n_hidden_layers=n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.seed=seed\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        train_tensor = torch.tensor(train_df.values.astype(np.float32),dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(self.scale_targets(target_df), dtype=torch.float32)\n",
    "        self.data = DataLoader(\n",
    "            torch.utils.data.TensorDataset(train_tensor, target_tensor),\n",
    "            batch_size=64,\n",
    "            shuffle=False\n",
    "        )\n",
    "        self.model=NeuralNet(\n",
    "            input_dimension=train_df.shape[1],\n",
    "            output_dimension=1,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            neurons=n_neurons,\n",
    "            retrain_seed=self.seed\n",
    "\n",
    "        ).to(self.device)\n",
    "        self.X=train_df\n",
    "        self.y=target_df\n",
    "\n",
    "        self.X_valid=X_valid\n",
    "        self.y_valid=y_valid\n",
    "\n",
    "        self.metadata_file=join(\"../logs\",\"task_3\" ,\"metadata.json\")\n",
    "\n",
    "    def compute_loss(self, inputs, targets, verbose=True):\n",
    "        preds=self.model(inputs)\n",
    "        # targets=targets/10000\n",
    "        res=targets-preds\n",
    "        loss=torch.mean(res**2)\n",
    "        # if verbose: print(\"Total loss: \", round(loss.item(), 4))\n",
    "        return loss\n",
    "\n",
    "    def save(self, loss_history):\n",
    "        filename=join(\"..\",\"models\", \"task_3\", datetime.datetime.now().strftime(\"%m-%d %H:%M:%S\")+\".pt\")\n",
    "        salient_info={}\n",
    "        salient_info[\"n_hidden_layers\"]=self.n_hidden_layers\n",
    "        salient_info[\"n_neurons\"]=self.n_neurons\n",
    "        salient_info[\"final_loss\"]=loss_history[-1]\n",
    "        salient_info[\"min_loss\"]=min(loss_history)\n",
    "        salient_info[\"model_path\"]=filename\n",
    "        salient_info[\"seed\"]=self.seed\n",
    "\n",
    "        torch.save(self.model.state_dict(),filename )\n",
    "        with open(self.metadata_file, \"a\") as f:\n",
    "            json.dump(salient_info, f)\n",
    "\n",
    "    def scale_targets(self, target_df):\n",
    "        y=target_df.values.astype(np.float32)\n",
    "        self.scaler=StandardScaler()\n",
    "        return(self.scaler.fit_transform(y.reshape(-1, 1)))\n",
    "    \n",
    "    # def validate(self, X,y):\n",
    "    #     self.model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "    #         targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "    #         loss=self.compute_loss(inputs, targets)\n",
    "    #         return loss\n",
    "    \n",
    "    def RMSE(self, validation=True):\n",
    "        if validation:\n",
    "            X=self.X_valid\n",
    "            y=self.y_valid\n",
    "        else:\n",
    "            X=self.X\n",
    "            y=self.y\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs=torch.tensor(X.values.astype(np.float32),dtype=torch.float32).to(self.device)\n",
    "            # targets=torch.tensor(self.scale_targets(y),dtype=torch.float32).to(self.device)\n",
    "            preds=self.model(inputs)\n",
    "            preds=self.scaler.inverse_transform(preds.cpu().numpy())\n",
    "            loss=np.sqrt(np.mean((y.values-preds)**2))\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (activation): ReLU()\n",
       "  (output_activation): Identity()\n",
       "  (input_layer): Linear(in_features=8, out_features=40, bias=True)\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0-2): 3 x Linear(in_features=40, out_features=40, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=40, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=NeuralNet(8,1,4,40,13)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_inp, num_epochs = num_epochs):\n",
    "    optimizer = torch.optim.RMSprop(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            # forward pass\n",
    "            outputs = model_inp(inputs)\n",
    "            # defining loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # computing gradients\n",
    "            loss.backward()\n",
    "            # accumulating running loss\n",
    "            running_loss += loss.item()\n",
    "            # updated weights based on computed gradients\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:    \n",
    "            print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %\n",
    "                  (epoch + 1, num_epochs, running_loss))\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[500] running accumulative loss across all batches: 736113207148544.000\n",
      "Epoch [21]/[500] running accumulative loss across all batches: 347004789063680.000\n",
      "Epoch [41]/[500] running accumulative loss across all batches: 155752993652736.000\n",
      "Epoch [61]/[500] running accumulative loss across all batches: 139697085710336.000\n",
      "Epoch [81]/[500] running accumulative loss across all batches: 127406961786880.000\n",
      "Epoch [101]/[500] running accumulative loss across all batches: 119016479227904.000\n",
      "Epoch [121]/[500] running accumulative loss across all batches: 113181500342272.000\n",
      "Epoch [141]/[500] running accumulative loss across all batches: 108844428427264.000\n",
      "Epoch [161]/[500] running accumulative loss across all batches: 105158417825792.000\n",
      "Epoch [181]/[500] running accumulative loss across all batches: 100869407145984.000\n",
      "Epoch [201]/[500] running accumulative loss across all batches: 96608305922048.000\n",
      "Epoch [221]/[500] running accumulative loss across all batches: 92689068687360.000\n",
      "Epoch [241]/[500] running accumulative loss across all batches: 89154272411648.000\n",
      "Epoch [261]/[500] running accumulative loss across all batches: 85651413221376.000\n",
      "Epoch [281]/[500] running accumulative loss across all batches: 82500779802624.000\n",
      "Epoch [301]/[500] running accumulative loss across all batches: 79486977736704.000\n",
      "Epoch [321]/[500] running accumulative loss across all batches: 76810460463104.000\n",
      "Epoch [341]/[500] running accumulative loss across all batches: 74397409181696.000\n",
      "Epoch [361]/[500] running accumulative loss across all batches: 72718618066944.000\n",
      "Epoch [381]/[500] running accumulative loss across all batches: 71313830232064.000\n",
      "Epoch [401]/[500] running accumulative loss across all batches: 70563715416064.000\n",
      "Epoch [421]/[500] running accumulative loss across all batches: 70061002137600.000\n",
      "Epoch [441]/[500] running accumulative loss across all batches: 69733220827136.000\n",
      "Epoch [461]/[500] running accumulative loss across all batches: 69131751546880.000\n",
      "Epoch [481]/[500] running accumulative loss across all batches: 68881092354048.000\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation:  69748.086\n",
      "Training:  69503.055\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(X_test)\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), y_test.detach().numpy()))\n",
    "print(\"Validation: \", err)\n",
    "print(\"Training: \", np.sqrt(mean_squared_error(model(X_train).detach().numpy(), y_train.detach().numpy())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
